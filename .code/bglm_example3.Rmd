---
title: "Bayesian GLM Part3"
author: "Murray Logan"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: show
    collapse: no
    df_print: paged
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    highlight: textmate
    theme: spacelab
    toc: yes
    toc_float: yes
    css: ../public/resources/ws_style.css
  pdf_document:
    df_print: default
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    highlight: tango
    latex_engine: xelatex
    number_sections: yes
    toc_depth: 2
  word_document:
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    highlight: tango
    toc: yes
    toc_depth: 2
output_dir: "docs"
documentclass: article
fontsize: 12pt
mainfont: Arial
mathfont: LiberationMono
monofont: DejaVu Sans Mono
classoption: a4paper
bibliography: ../public/resources/references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

# Preparations

Load the necessary libraries

```{r libraries, results='markdown', eval=TRUE}
library(rstanarm)   #for fitting models in STAN
library(brms)       #for fitting models in STAN
library(coda)       #for diagnostics
library(bayesplot)  #for diagnostics
library(ggmcmc)     #for MCMC diagnostics
library(DHARMa)     #for residual diagnostics
library(rstan)      #for interfacing with STAN
library(emmeans)    #for marginal means etc
library(broom)      #for tidying outputs
library(tidybayes)  #for more tidying outputs
library(ggeffects)  #for partial plots
library(tidyverse)  #for data wrangling etc
library(broom.mixed)#for summarising models
library(ggeffects)  #for partial effects plots
theme_set(theme_grey()) #put the default ggplot theme back
```

# Scenario

Here is a modified example from @Peake-1993-269.  @Peake-1993-269 investigated the relationship between the number of individuals of invertebrates living in amongst clumps of mussels on a rocky intertidal shore and the area of those mussel clumps.

![](../public/resources/mussels.jpg)

Format of peakquinn.csv data files

| AREA      | INDIV   |
| --------- | ------- |
| 516.00    | 18      |
| 469.06    | 60      |
| 462.25    | 57      |
| 938.60    | 100     |
| 1357.15   | 48      |
| \...      | \...    |

----------- --------------------------------------------------------------
**AREA**    Area of mussel clump mm^2^ - Predictor variable
**INDIV**   Number of individuals found within clump - Response variable
----------- --------------------------------------------------------------



The aim of the analysis is to investigate the relationship between mussel clump area and the number of non-mussel invertebrate individuals supported in the mussel clump.

# Read in the data

```{r readData, results='markdown', eval=TRUE}
peake = read_csv('../public/data/peakquinn.csv', trim_ws=TRUE)
glimpse(peake)
```

# Exploratory data analysis

<div class='HIDDEN'>
When exploring these data as part of a frequentist analysis, exploratory data
analysis revealed that the both the response (counds of individuals) and
predictor (mussel clump area) were skewed and the relationship between raw
counds and mussel clump area was not linear.  Furthermore, there was strong
evidence of a relationship between mean and variance. Normalising both reponse
and predictor addressed these issues.  However, rather than log transform the
response, it was considered more appropriate to model against a distribution
that used a logarithmic link function.

The individual observations here ($y_i$) are the observed number of (non mussel
individuals found in mussel clump $i$.  As a count, these might be expected to
follow a Poisson (or perhaps negative binomial) distribution.  In the case of a
negative binomial, the observed count for any given mussel clump area are
expected to be drawn from a negative binomial distribution with a mean of
$\lambda_i$.  All the negative binomial distributions are expected to share the
same degree of dispersion ($\theta$) - that is, the degree to which the
inhabitants of mussell clumps aggregate together (or any other reason for
overdispersion) is independent of mussel clump area and can be estimated as a
constant across all populations.

The natural log of the expected values ($\lambda_i$) is modelled against a
linear predictor that includes an intercept ($\beta_0$) and a slope ($\beta_1$)
associated with the natural log of mussel area.

The priors on $\beta_0$ and $\beta_1$ should be on the natura log scale (since
this will be the scale of the parameters).  As starting points, we will consider
the following priors:

- $\beta_0$: Normal prior centered at 0 with a variance of 5
- $\beta_1$: Normal prior centered at 0 with a variance of 2
- $\theta$: Exponential prior with rate 1

</div>

Model formula:
$$
\begin{align}
y_i &\sim{} \mathcal{NB}(\lambda_i, \theta)\\
ln(\lambda_i) &= \beta_0 + \beta_1 ln(x_i)\\
\beta_0 & \sim\mathcal{N}(0,5)\\
\beta_1 & \sim\mathcal{N}(0,2)\\
\theta &\sim{} \mathcal{Exp}(1)
\end{align}
$$

# Fit the model {.tabset .tabset-faded}

<div class='HIDDEN'>

## frequentist

```{r lm, results='markdown', eval=TRUE, hidden=TRUE}
summary(glm(INDIV ~ log(AREA), data=peake, family=poisson()))
summary(MASS::glm.nb(INDIV ~ log(AREA), data=peake))
```

## rstanarm {.tabset .tabset-pills}

### Using default priors

In `rstanarm`, the default priors are designed to be weakly informative. They
are chosen to provide moderate regularlization (to help prevent overfitting) and
help stabalise the computations.

```{r fitModel1a, results='markdown', eval=TRUE, hidden=TRUE, cache=TRUE}
peake.rstanarm = stan_glm(INDIV ~ log(AREA), data=peake,
                          family=poisson(), 
                         iter = 5000, warmup = 1000,
                         chains = 3, thin = 5, refresh = 0)
```

```{r fitModel1b, results='markdown', eval=TRUE, hidden=TRUE, cache=FALSE}
prior_summary(peake.rstanarm)
```

This tells us:

- for the intercept, it is using a normal prior with a mean of 0 and a
  standard deviation of 2.5.  These are the defaults for all non-Gaussian intercepts.

- for the coefficients (in this case, just the slope), the default prior is a
normal prior centered around 0 with a standard deviation of 2.5.  For Poisson
models, this is then
adjusted for the scale of the data by dividing the 2.5 by the
standard deviation of the (in this case log) predictor
(then rounded). 

```{r fitModel1d, results='markdown', eval=TRUE, hidden=TRUE, cache=FALSE}
2.5/sd(log(peake$AREA))
```

-  there is no auxillary parameter and thus there is no auxillary prior 


### Assessing priors

One way to assess the priors is to have the MCMC sampler sample purely from the
prior predictive distribution without conditioning on the observed data.  Doing
so provides a glimpse at the range of predictions possible under the priors.  On
the one hand, wide ranging preditions would ensure that the priors are unlikely
to influence the actual preditions once they are conditioned on the data.  On
the other hand, if they are too wide, the sampler is being permitted to traverse
into regions of parameter space that are not logically possible in the context
of the actual underlying ecological context.  Not only could this mean that
illogical parameter estimates are possible, when the sampler is traversing
regions of paramter space that are not supported by the actual data, the sampler
can become unstable and have difficulty.

We can draw from the prior predictive distribution instead of conditioning on
the response, by updating the model and indicating `prior_PD=TRUE`.  After
refittin the model in this way, we can plot the predictions to gain insights
into the range of predictions supported by the priors alone.

```{r fitModel1f, results='markdown', eval=TRUE, hidden=TRUE, cache=TRUE}
peake.rstanarm1 <- update(peake.rstanarm,  prior_PD=TRUE)
```
```{r fitModel1g, results='markdown', eval=TRUE, hidden=TRUE, cache=FALSE}
ggemmeans(peake.rstanarm1,  ~AREA) %>% plot(add.data=TRUE)
```
 
**Conclusions:**

- we see that the range of predictions are very wide and the slope could range
  from strongly negative to strongly positive.

### Defining priors

The following link provides some guidance about defining priors.
[https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations]

When defining our own priors, we typically do not want them to be scaled.

If we wanted to define our own priors that were less vague, yet still not likely
to bias the outcomes, we could try the following priors (mainly plucket out of
thin air):

- $\beta_0$: normal centered at 0 with a standard deviation of 5
- $\beta_1$: normal centered at 0 with a standard deviation of 2

I will also overlay the raw data for comparison.

```{r fitModel1h, results='markdown', eval=TRUE, hidden=TRUE, cache=TRUE}
peake.rstanarm2= stan_glm(INDIV ~ log(AREA), data=peake,
                          family=poisson(), 
                          prior_intercept = normal(0, 5, autoscale=FALSE),
                          prior = normal(0, 2, autoscale=FALSE),
                          prior_PD=TRUE, 
                          iter = 5000, warmup = 1000,
                          chains = 3, thin = 5, refresh = 0
                          )
```

```{r fitModel1i, results='markdown', eval=TRUE, hidden=TRUE, cache=FALSE}
ggemmeans(peake.rstanarm2,  ~AREA) %>%
  plot(add.data=TRUE)
```

Now lets refit, conditioning on the data.

```{r fitModel1j, results='markdown', eval=TRUE, hidden=TRUE, cache=TRUE}
peake.rstanarm3= update(peake.rstanarm2,  prior_PD=FALSE)  
```

### Plotting prior and posterior

```{r modelFit1k, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
posterior_vs_prior(peake.rstanarm3, color_by='vs', group_by=TRUE,
                   facet_args=list(scales='free_y'))
```

**Conclusions:**

- in each case, the prior is substantially wider than the posterior, suggesting
  that the posterior is not biased towards the prior.
  
```{r modelFit1l, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
ggemmeans(peake.rstanarm3,  ~AREA) %>% plot(add.data=TRUE)
```

## brms {.tabset .tabset-pills}

### Using default priors

In `brms`, the default priors are designed to be weakly informative.  They are
chosen to provide moderate regularlization (to help prevent overfitting) and
help stabalise the computations.

Unlike `rstanarm`, `brms` models must be compiled before they start sampling.
For most models, the compilation of the stan code takes around 45 seconds.

```{r fitModel2a, results='markdown', eval=TRUE, hidden=TRUE, cache=TRUE}
peake.brm = brm(bf(INDIV ~ log(AREA), family=poisson()),
                data=peake,
                iter = 5000, warmup = 1000,
                chains = 3, thin = 5, refresh = 0)
```

```{r fitModel2b, results='markdown', eval=TRUE, hidden=TRUE, cache=FALSE, paged.print=FALSE,tidy.opts = list(width.cutoff = 80), echo=2}
options(width=100)
prior_summary(peake.brm)
options(width=80)
```

This tells us:

- for the intercept, it is using a student t (flatter normal) prior with a mean of 5.8 and a
  standard deviation of 2.5. The mean of this prior is based on the median of
  the log-transformed response and the standard deviation of 2.5 is the default minimum.
  
```{r fitModel2c, results='markdown', eval=TRUE, hidden=TRUE, cache=FALSE, paged.print=FALSE,tidy.opts = list(width.cutoff = 80), echo=2}
median(log(peake$INDIV))
mad(log(peake$INDIV))
```

- for the beta coefficients (in this case, just the slope), the default prior is
  a inproper flat prior. A flat prior essentially means that any value between
  negative infinity and positive infinity are equally likely. Whilst this might
  seem reckless, in practice, it seems to work reasonably well for non-intercept
  beta parameters.

- there is no sigma parameter in a binomial model and therefore there are no
  additional priors.
  
### Assessing priors

One way to assess the priors is to have the MCMC sampler sample purely from the
prior predictive distribution without conditioning on the observed data.  Doing
so provides a glimpse at the range of predictions possible under the priors.  On
the one hand, wide ranging preditions would ensure that the priors are unlikely
to influence the actual preditions once they are conditioned on the data.  On
the other hand, if they are too wide, the sampler is being permitted to traverse
into regions of parameter space that are not logically possible in the context
of the actual underlying ecological context.  Not only could this mean that
illogical parameter estimates are possible, when the sampler is traversing
regions of paramter space that are not supported by the actual data, the sampler
can become unstable and have difficulty.

In `brms`, we can inform the sampler to draw from the prior predictive
distribution instead of conditioning on the response, by running the model with
the `sample_prior='only'` argument.  Unfortunately, this cannot be applied when
there are flat priors (since the posteriors will necessarily extend to negative
and positive infinity).  Therefore, in order to use this useful routine, we need
to make sure that we have defined a proper prior for all parameters.


```{r fitModel2d, results='markdown', eval=TRUE, hidden=TRUE, cache=TRUE}
peake.brm1 = brm(bf(INDIV ~ log(AREA), family=poisson()),
                 data=peake,
                prior=c(
                  prior(normal(0, 2), class='b')), 
                sample_prior = 'only', 
                iter = 5000, warmup = 1000,
                chains = 3, thin = 5, refresh = 0)
```


```{r fitModel2e, results='markdown', eval=TRUE, hidden=TRUE, cache=FALSE}
ggemmeans(peake.brm1,  ~AREA) %>% plot(add.data=TRUE)
conditional_effects(peake.brm1) %>%  plot(points=TRUE)
```

**Conclusions:**

- we see that the range of predictions is faily wide and the slope could range
  from strongly negative to strongly positive.

### Defining priors

The following link provides some guidance about defining priors.
[https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations]

When defining our own priors, we typically do not want them to be scaled.

If we wanted to define our own priors that were less vague, yet still not likely
to bias the outcomes, we could try the following priors (mainly plucket out of
thin air):

- $\beta_0$: normal centered at 0 with a standard deviation of 5
- $\beta_1$: normal centered at 0 with a standard deviation of 2

I will also overlay the raw data for comparison.


```{r fitModel2h, results='markdown', eval=TRUE, hidden=TRUE, cache=TRUE}
peake.brm2 = brm(bf(INDIV ~ log(AREA), family=poisson()),
                 data=peake,
                 prior=c(
                   prior(normal(0, 5),  class='Intercept'),
                   prior(normal(0, 2), class='b')
                 ), 
                 sample_prior = 'only', 
                 iter = 5000, warmup = 1000,
                 chains = 3, thin = 5, refresh = 0)
```

```{r fitModel2i, results='markdown', eval=TRUE, hidden=TRUE, cache=FALSE}
ggemmeans(peake.brm2,  ~AREA) %>%
  plot(add.data=TRUE)
```

```{r fitModel2j, results='markdown', eval=TRUE, hidden=TRUE, cache=TRUE}
peake.brm3 <- update(peake.brm2,  sample_prior=TRUE, refresh=0)
```

### Plotting prior and posterior


```{r fitModel2k, results='markdown', eval=TRUE, hidden=TRUE, cache=FALSE}
peake.brm3 %>% get_variables()
peake.brm3 %>%
  posterior_samples %>%
  select(-`lp__`) %>%
  gather %>%
  mutate(Type=ifelse(str_detect(key, 'prior'), 'Prior', 'b'),
         Class=ifelse(str_detect(key, 'Intercept'),  'Intercept',
               ifelse(str_detect(key, 'b'),  'b', NA))) %>%
  ggplot(aes(x=Type,  y=value)) +
  stat_pointinterval()+
  facet_wrap(~Class,  scales='free')
```

### Exploring the stan code

```{r fitModel2l, results='markdown', eval=TRUE, hidden=TRUE, cache=FALSE}
standata(peake.brm3)
stancode(peake.brm3)
```

</div>


# MCMC sampling diagnostics {.tabset .tabset-faded}


<div class='HIDDEN'>
In addition to the regular model diagnostics checking, for Bayesian analyses, it
is also necessary to explore the MCMC sampling diagnostics to be sure that the
chains are well mixed and have converged on a stable posterior.

There are a wide variety of tests that range from the big picture, overal chain
characteristics to the very specific detailed tests that allow the experienced
modeller to drill down to the very fine details of the chain behaviour.
Furthermore, there are a multitude of packages and approaches for exploring
these diagnostics.


## rstanarm {.tabset .tabset-pills}

### bayesplot

The `bayesplot` package offers a range of MCMC diagnostics as well as Posterior
Probability Checks (PPC), all of which have a convenient `plot()` interface.
Lets start with the MCMC diagnostics.

```{r modelValidation1a, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
available_mcmc()
```

Of these, we will focus on:

- mcmc_trace: this plots the estimates of each parameter over the post-warmup
  length of each MCMC chain.  Each chain is plotted in a different shade of
  blue, with each parameter in its own facet.  Ideally, each **trace** should
  just look like noise without any discernible drift and each of the traces for
  a specific parameter should look the same (i.e, should not be displaced above
  or below any other trace for that parameter).

```{r modelValidation1b, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
plot(peake.rstanarm3, plotfun='mcmc_trace')
```
  
   The chains appear well mixed and very similar
   
- acf (autocorrelation function): plots the autocorrelation between successive
  MCMC sample lags for each parameter and each chain
  
```{r modelValidation1c, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
plot(peake.rstanarm3, 'acf_bar')
```

   There is no evidence of autocorrelation in the MCMC samples

- Rhat: Rhat is a measure of convergence between the chains.  The closer the
  values are to 1, the more the chains have converged.  Values greater than 1.05
  indicate a lack of convergence.  There will be an Rhat value for each
  parameter estimated.

```{r modelValidation1d, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
plot(peake.rstanarm3, 'rhat_hist')
```

  All Rhat values are below 1.05, suggesting the chains have converged.
  
- neff (number of effective samples): the ratio of the number of effective
  samples (those not rejected by the sampler) to the number of samples provides
  an indication of the effectiveness (and efficiency) of the MCMC sampler.
  Ratios that are less than 0.5 for a parameter suggest that the sampler spent
  considerable time in difficult areas of the sampling domain and rejected more
  than half of the samples (replacing them with the previous effective sample).  
  
  If the ratios are low, tightening the priors may help.
  
```{r modelValidation1e, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
plot(peake.rstanarm3, 'neff_hist')
```

  Ratios all very high.

<details><summary>More diagnostics</summary>
```{r Validation1f, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
plot(peake.rstanarm3, 'combo')
plot(peake.rstanarm3, 'violin')
```
</details>


### stan plots

The `rstan` package offers a range of MCMC diagnostics.
Lets start with the MCMC diagnostics.

Of these, we will focus on:

- stan_trace: this plots the estimates of each parameter over the post-warmup
  length of each MCMC chain. Each chain is plotted in a different colour, with
  each parameter in its own facet. Ideally, each **trace** should just look like
  noise without any discernible drift and each of the traces for a specific
  parameter should look the same (i.e, should not be displaced above or below
  any other trace for that parameter).
  
```{r modelValidation1g, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
stan_trace(peake.rstanarm3)
```

   The chains appear well mixed and very similar
   
- stan_acf (autocorrelation function): plots the autocorrelation between successive
  MCMC sample lags for each parameter and each chain
  
```{r modelValidation1h, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
stan_ac(peake.rstanarm3) 
```

   There is no evidence of autocorrelation in the MCMC samples

- stan_rhat: Rhat is a **scale reduction factor** measure of convergence between the chains.  The closer the
  values are to 1, the more the chains have converged.  Values greater than 1.05
  indicate a lack of convergence.  There will be an Rhat value for each
  parameter estimated.

```{r modelValidation1i, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
stan_rhat(peake.rstanarm3) 
```

  All Rhat values are below 1.05, suggesting the chains have converged.
  
- stan_ess (number of effective samples): the ratio of the number of effective
  samples (those not rejected by the sampler) to the number of samples provides
  an indication of the effectiveness (and efficiency) of the MCMC sampler.
  Ratios that are less than 0.5 for a parameter suggest that the sampler spent
  considerable time in difficult areas of the sampling domain and rejected more
  than half of the samples (replacing them with the previous effective sample).  
  
  If the ratios are low, tightening the priors may help.
  
```{r modelValidation1j, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
stan_ess(peake.rstanarm3)
```

  Ratios all very high.

```{r modelValidation1k, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
stan_dens(peake.rstanarm3, separate_chains = TRUE)
```

### ggmcmc

The `ggmean` package also has a set of MCMC diagnostic functions.
Lets start with the MCMC diagnostics.

Of these, we will focus on:

- ggs_traceplot: this plots the estimates of each parameter over the post-warmup
  length of each MCMC chain. Each chain is plotted in a different colour, with
  each parameter in its own facet. Ideally, each **trace** should just look like
  noise without any discernible drift and each of the traces for a specific
  parameter should look the same (i.e, should not be displaced above or below
  any other trace for that parameter).

```{r modelValidation1l, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
peake.ggs <- ggs(peake.rstanarm3)
ggs_traceplot(peake.ggs)
```

   The chains appear well mixed and very similar
   
- gss_autocorrelation (autocorrelation function): plots the autocorrelation between successive
  MCMC sample lags for each parameter and each chain
  
```{r modelValidation1m, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
ggs_autocorrelation(peake.ggs)
```

   There is no evidence of autocorrelation in the MCMC samples

- stan_rhat: Rhat is a **scale reduction factor** measure of convergence between the chains.  The closer the
  values are to 1, the more the chains have converged.  Values greater than 1.05
  indicate a lack of convergence.  There will be an Rhat value for each
  parameter estimated.

```{r modelValidation1n, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
ggs_Rhat(peake.ggs)
```

  All Rhat values are below 1.05, suggesting the chains have converged.
  
- stan_ess (number of effective samples): the ratio of the number of effective
  samples (those not rejected by the sampler) to the number of samples provides
  an indication of the effectiveness (and efficiency) of the MCMC sampler.
  Ratios that are less than 0.5 for a parameter suggest that the sampler spent
  considerable time in difficult areas of the sampling domain and rejected more
  than half of the samples (replacing them with the previous effective sample).  
  
  If the ratios are low, tightening the priors may help.
  
```{r modelValidation1o, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
ggs_effective(peake.ggs)
```

  Ratios all very high.

<details><summary>More diagnostics</summary>
```{r modelValidation1p, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
ggs_crosscorrelation(peake.ggs)
```

```{r modelValidation1q, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
ggs_grb(peake.ggs)
```
</details>

## brms {.tabset .tabset-pills}

### bayesplot

The `bayesplot` package offers a range of MCMC diagnostics as well as Posterior
Probability Checks (PPC), all of which have a convenient `plot()` interface.
Lets start with the MCMC diagnostics.

```{r modelValidation2a, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
available_mcmc()
```

Of these, we will focus on:

- trace: this plots the estimates of each parameter over the post-warmup
  length of each MCMC chain.  Each chain is plotted in a different shade of
  blue, with each parameter in its own facet.  Ideally, each **trace** should
  just look like noise without any discernible drift and each of the traces for
  a specific parameter should look the same (i.e, should not be displaced above
  or below any other trace for that parameter).

```{r modelValidation2b, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
mcmc_plot(peake.brm3, type='trace')
```
  
   The chains appear well mixed and very similar
   
- acf_bar (autocorrelation function): plots the autocorrelation between successive
  MCMC sample lags for each parameter and each chain
  
```{r modelValidation2c, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
mcmc_plot(peake.brm3, type='acf_bar')
```

   There is no evidence of autocorrelation in the MCMC samples

- rhat_hist: Rhat is a **scale reduction factor** measure of convergence between the chains.  The closer the
  values are to 1, the more the chains have converged.  Values greater than 1.05
  indicate a lack of convergence.  There will be an Rhat value for each
  parameter estimated.

```{r modelValidation2d, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
mcmc_plot(peake.brm3, type='rhat_hist')
```

  All Rhat values are below 1.05, suggesting the chains have converged.
  
- neff_hist (number of effective samples): the ratio of the number of effective
  samples (those not rejected by the sampler) to the number of samples provides
  an indication of the effectiveness (and efficiency) of the MCMC sampler.
  Ratios that are less than 0.5 for a parameter suggest that the sampler spent
  considerable time in difficult areas of the sampling domain and rejected more
  than half of the samples (replacing them with the previous effective sample).  
  
  If the ratios are low, tightening the priors may help.
  
```{r modelValidation2e, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
mcmc_plot(peake.brm3, type='neff_hist')
```

  Ratios all very high.

<details><summary>More diagnostics</summary>
```{r modelValidation2f, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
mcmc_plot(peake.brm3, type='combo')
mcmc_plot(peake.brm3, type='violin')
```
</details>

### stan plots

The `rstan` package offers a range of MCMC diagnostics.
Lets start with the MCMC diagnostics.

Of these, we will focus on:

- stan_trace: this plots the estimates of each parameter over the post-warmup
  length of each MCMC chain. Each chain is plotted in a different colour, with
  each parameter in its own facet. Ideally, each **trace** should just look like
  noise without any discernible drift and each of the traces for a specific
  parameter should look the same (i.e, should not be displaced above or below
  any other trace for that parameter).
  
```{r modelValidation2g, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
stan_trace(peake.brm3$fit)
```

   The chains appear well mixed and very similar
   
- stan_acf (autocorrelation function): plots the autocorrelation between successive
  MCMC sample lags for each parameter and each chain
  
```{r modelValidation2h, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
stan_ac(peake.brm3$fit) 
```

   There is no evidence of autocorrelation in the MCMC samples

- stan_rhat: Rhat is a **scale reduction factor** measure of convergence between the chains.  The closer the
  values are to 1, the more the chains have converged.  Values greater than 1.05
  indicate a lack of convergence.  There will be an Rhat value for each
  parameter estimated.

```{r modelValidation2i, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
stan_rhat(peake.brm3$fit) 
```

  All Rhat values are below 1.05, suggesting the chains have converged.
  
- stan_ess (number of effective samples): the ratio of the number of effective
  samples (those not rejected by the sampler) to the number of samples provides
  an indication of the effectiveness (and efficiency) of the MCMC sampler.
  Ratios that are less than 0.5 for a parameter suggest that the sampler spent
  considerable time in difficult areas of the sampling domain and rejected more
  than half of the samples (replacing them with the previous effective sample).  
  
  If the ratios are low, tightening the priors may help.
  
```{r modelValidation2j, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
stan_ess(peake.brm3$fit)
```

  Ratios all very high.

```{r modelValidation2k, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
stan_dens(peake.brm3$fit, separate_chains = TRUE)
```

### ggmcmc

The `ggmean` package also has a set of MCMC diagnostic functions.
Lets start with the MCMC diagnostics.

Of these, we will focus on:

- ggs_traceplot: this plots the estimates of each parameter over the post-warmup
  length of each MCMC chain. Each chain is plotted in a different colour, with
  each parameter in its own facet. Ideally, each **trace** should just look like
  noise without any discernible drift and each of the traces for a specific
  parameter should look the same (i.e, should not be displaced above or below
  any other trace for that parameter).

```{r modelValidation2l, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=7}
peake.ggs <- ggs(peake.brm3)
ggs_traceplot(peake.ggs)
```

   The chains appear well mixed and very similar
   
- gss_autocorrelation (autocorrelation function): plots the autocorrelation between successive
  MCMC sample lags for each parameter and each chain
  
```{r modelValidation2m, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=7}
ggs_autocorrelation(peake.ggs)
```

   There is no evidence of autocorrelation in the MCMC samples

- stan_rhat: Rhat is a **scale reduction factor** measure of convergence between the chains.  The closer the
  values are to 1, the more the chains have converged.  Values greater than 1.05
  indicate a lack of convergence.  There will be an Rhat value for each
  parameter estimated.

```{r modelValidation2n, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
ggs_Rhat(peake.ggs)
```

  All Rhat values are below 1.05, suggesting the chains have converged.
  
- stan_ess (number of effective samples): the ratio of the number of effective
  samples (those not rejected by the sampler) to the number of samples provides
  an indication of the effectiveness (and efficiency) of the MCMC sampler.
  Ratios that are less than 0.5 for a parameter suggest that the sampler spent
  considerable time in difficult areas of the sampling domain and rejected more
  than half of the samples (replacing them with the previous effective sample).  
  
  If the ratios are low, tightening the priors may help.
  
```{r modelValidation2o, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
ggs_effective(peake.ggs)
```

  Ratios all very high.

<details><summary>More diagnostics</summary>
```{r modelValidation2p, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
ggs_crosscorrelation(peake.ggs)
```

```{r modelValidation2q, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
ggs_grb(peake.ggs)
```
</details>


</div>

# Model validation {.tabset .tabset-faded}

<div class='HIDDEN'>

## rstanarm {.tabset .tabset-pills}

### pp check
Post predictive checks provide additional diagnostics about the fit of the
model.  Specifically, they provide a comparison between predictions drawn from
the model and the observed data used to train the model.

```{r modelValidation3a, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
available_ppc()
```

- dens_overlay: plots the density distribution of the observed data (black line)
overlayed ontop of 50 density distributions generated from draws from the model
(light blue).  Ideally, the 50 realisations should be roughly consistent with
the observed data.

```{r modelValidation3b, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
pp_check(peake.rstanarm3,  plotfun='dens_overlay')
```

The model draws appear deviate from the observed data.

- error_scatter_avg: this plots the observed values against the average
  residuals. Similar to a residual plot, we do not want to see any patterns in
  this plot.  There is some pattern remaining in these residuals.

```{r modelValidation3c, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
pp_check(peake.rstanarm3, plotfun='error_scatter_avg')
```

The predictive error seems to be related to the predictor - the model performs
poorest at higher mussel clump areas.


- error_scatter_avg_vs_x: this is similar to a regular residual plot and as such
  should be interpreted as such.  Again, this is not interpretable for binary data.

```{r modelValidation3d, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
pp_check(peake.rstanarm3, x=peake$AREA, plotfun='error_scatter_avg_vs_x')
```

- intervals:  plots the observed data overlayed ontop of posterior predictions
associated with each level of the predictor.  Ideally, the observed data should
all fall within the predictive intervals.


```{r modelValidation3e, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
pp_check(peake.rstanarm3, x=peake$AREA, plotfun='intervals')
```

The modelled preditions seem to underestimate the uncertainty with increasing
mussel clump area.

- ribbon: this is just an alternative way of expressing the above plot.

```{r modelValidation3f, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
pp_check(peake.rstanarm3, x=peake$AREA, plotfun='ribbon')
```

The `shinystan` package allows the full suite of MCMC diagnostics and posterior
predictive checks to be accessed via a web interface.

```{r modelValidation3g, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
#library(shinystan)
#launch_shinystan(peake.rstanarm3)
```

### DHARMa residuals

DHARMa residuals provide very useful diagnostics.  Unfortunately, we cannot
directly use the `simulateResiduals()` function to generate the simulated
residuals.  However, if we are willing to calculate some of the components
ourself, we can still obtain the simulated residuals from the fitted stan model.

We need to supply:

- simulated (predicted) responses associated with each observation.
- observed values
- fitted (predicted) responses (averaged) associated with each observation

```{r modelValidation4a, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
preds <- posterior_predict(peake.rstanarm3,  nsamples=250,  summary=FALSE)
peake.resids <- createDHARMa(simulatedResponse = t(preds),
                            observedResponse = peake$INDIV,
                            fittedPredictedResponse = apply(preds, 2, median),
                            integerResponse = TRUE)
plot(peake.resids)
```

**Conclusions:**

- the simulated residuals suggest a general lack of fit due to overdispersion
and outliers 
- perhaps we should explore a negative binomial model

## brms {.tabset .tabset-pills}

### pp check
Post predictive checks provide additional diagnostics about the fit of the
model.  Specifically, they provide a comparison between predictions drawn from
the model and the observed data used to train the model.

```{r modelValidation5a, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
available_ppc()
```

- dens_overlay: plots the density distribution of the observed data (black line)
overlayed ontop of 50 density distributions generated from draws from the model
(light blue).  Ideally, the 50 realisations should be roughly consistent with
the observed data.

```{r modelValidation5b, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
pp_check(peake.brm3,  type='dens_overlay')
```

The model draws appear deviate from the observed data.

- error_scatter_avg: this plots the observed values against the average
  residuals. Similar to a residual plot, we do not want to see any patterns in
  this plot.  There is some pattern remaining in these residuals.

```{r modelValidation5c, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
pp_check(peake.brm3, type='error_scatter_avg')
```

The predictive error seems to be related to the predictor - the model performs
poorest at higher mussel clump areas.


- error_scatter_avg_vs_x: this is similar to a regular residual plot and as such
  should be interpreted as such.  Again, this is not interpretable for binary data.

```{r modelValidation5d, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
pp_check(peake.brm3, x='AREA', type='error_scatter_avg_vs_x')
```

- intervals:  plots the observed data overlayed ontop of posterior predictions
associated with each level of the predictor.  Ideally, the observed data should
all fall within the predictive intervals.


```{r modelValidation5e, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
pp_check(peake.brm3, x='AREA', type='intervals')
```

The modelled preditions seem to underestimate the uncertainty with increasing
mussel clump area.

- ribbon: this is just an alternative way of expressing the above plot.

```{r modelValidation5f, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
pp_check(peake.brm3, x='AREA', type='ribbon')
```

The `shinystan` package allows the full suite of MCMC diagnostics and posterior
predictive checks to be accessed via a web interface.

```{r modelValidation5g, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
#library(shinystan)
#launch_shinystan(peake.brm3)
```

### DHARMa residuals

DHARMa residuals provide very useful diagnostics.  Unfortunately, we cannot
directly use the `simulateResiduals()` function to generate the simulated
residuals.  However, if we are willing to calculate some of the components
ourself, we can still obtain the simulated residuals from the fitted stan model.

We need to supply:

- simulated (predicted) responses associated with each observation.
- observed values
- fitted (predicted) responses (averaged) associated with each observation

```{r modelValidation6a, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
preds <- posterior_predict(peake.brm3,  nsamples=250,  summary=FALSE)
peake.resids <- createDHARMa(simulatedResponse = t(preds),
                            observedResponse = peake$INDIV,
                            fittedPredictedResponse = apply(preds, 2, median),
                            integerResponse = TRUE)
plot(peake.resids)
```

**Conclusions:**

- the simulated residuals suggest a general lack of fit due to overdispersion
and outliers 
- perhaps we should explore a negative binomial model

</div>

# Fit Negative Binomial model {.tabset .tabset-faded}

<div class='HIDDEN'>

## rstanarm {.tabset .tabset-pills}

### Fit the model
```{r fitNBModel1a, results='markdown', eval=TRUE, hidden=TRUE, cache=TRUE}
peake.rstanarm4= stan_glm(INDIV ~ log(AREA), data=peake,
                          family=neg_binomial_2(), 
                          prior_intercept = normal(0, 5, autoscale=FALSE),
                          prior = normal(0, 2, autoscale=FALSE),
                          prior_aux = rstanarm::exponential(rate=1, autoscale=FALSE), 
                          iter = 5000, warmup = 1000,
                          chains = 3, thin = 5, refresh = 0
                          )
```

### Plotting prior and posterior

```{r fitNBModel1b, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
posterior_vs_prior(peake.rstanarm4, color_by='vs', group_by=TRUE,
                   facet_args=list(scales='free_y'))
```

**Conclusions:**

- in each case, the prior is substantially wider than the posterior, suggesting
  that the posterior is not biased towards the prior.
  
```{r fitNBModel1c, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
ggemmeans(peake.rstanarm4,  ~AREA) %>% plot(add.data=TRUE)
```

### MCMC chain and posterior checks
```{r fitNBModel1d, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
plot(peake.rstanarm4, plotfun='mcmc_trace')
plot(peake.rstanarm4, 'acf_bar')
plot(peake.rstanarm4, 'rhat_hist')
plot(peake.rstanarm4, 'neff_hist')
pp_check(peake.rstanarm4, x=peake$AREA, plotfun='error_scatter_avg_vs_x')
pp_check(peake.rstanarm4, x=peake$AREA, plotfun='intervals')
```

### DHARMa residuals

```{r fitNBModel1e, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
preds <- posterior_predict(peake.rstanarm4,  nsamples=250,  summary=FALSE)
peake.resids <- createDHARMa(simulatedResponse = t(preds),
                            observedResponse = peake$INDIV,
                            fittedPredictedResponse = apply(preds, 2, median),
                            integerResponse = TRUE)
plot(peake.resids)
```

### LOOIC

Different models can be compared via information criterion.
LOO (Leave One Out) IC is similar to AIC except that AIC does not consider priors
and assumes that the posterior likelihood is multivariate normal. LOO AIC does
not and integrates over all uncertainty.

- ELPD: is the theoretical expected log pointwise predictive density for a
  new dataset and can be estimated via leave-one-out cross-validation.
- `elpd_loo`: is the Bayesian LOO estimate of ELPD
- `SE` of `elpd_loo`: is the Monte Carlo standard error is the estimate for the
  computational accuracy of MCMC and importance sampling used to compute
  `elpd_loo`
- `p_loo`: is the difference between `elpd_loo` and the estimated from
  non-cross validated ELPD and therefore is a measure of the relative extra
  difficulty of predicting new data compared to the observed data.  It can also
  be a meaure of the effective number of parameters:
  - in a well behaved model, `p_loo` will be less than the number of
    estimated parameters and the total sample size.
  - in a misspecified model, `p_loo` will be greater than the number of
    estimated parameters and the total sample size.
- `Pareto k`: is a relative measure of the influence of each observation as well
  as the accuracy with which the associated leave one out calculations can be
  estimated.
  - when `k < 0.5`: the corresponding components can be accurately estimated
  - when `0.5 < k < 0.7`: the accuracy is lower but still acceptable
  - when `k>0.7`: the accuracy is too low and `elpd_loo` is unreliable.  This
    can also suggest a misspecified model.

More information about the above can be gleaned from [https://mc-stan.org/loo/reference/loo-glossary.html].


Start with the Poisson model
```{r fitNBModel1f, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
(peake.rstanarm3.loo =loo(peake.rstanarm3))
```

**Conclusions:**

- `p_loo` is substantially higher than the number of estimated parameters
- there are some `Pareto k` values identified as bad or even very bad

Now for the Negative Binomial model
```{r fitNBModel1g, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
(peake.rstanarm4.loo =loo(peake.rstanarm4))
```

**Conclusions:**

- `p_loo` is lower than the number of estimated parameters
- there are no bad `Pareto k` values

We can also compare the models.

The difference in Expected Log Pointwise Probability Density (`elpd_dff`)
computes the difference in `elpd_loo` of two models (expressed relative to the
higher model).
The difference in `elpd_diff` will be negative if the
expected out-of-sample predictive accuracy of the first model is
higher. If the difference is be positive then the second model is
preferred.

Note, the above assumes that both models are valid.  In the current case, we
know that the Poisson model was overdispersed and thus it should not be a
genuine candidate.  Nevertheless, we will compare the Poisson to the Negative
Binomial for illustrative purposes.

```{r fitNBModel1h, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
loo_compare(peake.rstanarm3.loo, peake.rstanarm4.loo)
```

**Conclusions:**

- the `elpd_diff` is negative, indicating that the first model (Negative
  Binomial) is better.

## brms {.tabset .tabset-pills}

### Fit the model
```{r fitNBModel2a, results='markdown', eval=TRUE, hidden=TRUE, cache=TRUE}
peake.brm4 = brm(bf(INDIV ~ log(AREA), family=negbinomial()),
                 data=peake,
                 prior=c(
                   prior(normal(0, 5),  class='Intercept'),
                   prior(normal(0, 2), class='b'),
                   prior(gamma(0.01, 0.01), class='shape')
                 ),
                 sample_prior=TRUE, 
                 iter = 5000, warmup = 1000,
                 chains = 3, thin = 5, refresh = 0)
                          
```

### Plotting prior and posterior

```{r fitNBModel2b, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
peake.brm4 %>%
  posterior_samples %>%
  select(-`lp__`) %>%
  gather %>%
  mutate(Type=ifelse(str_detect(key, 'prior'), 'Prior', 'b'),
         Class=ifelse(str_detect(key, 'Intercept'),  'Intercept',
               ifelse(str_detect(key, 'b'),  'b', 'shape'))) %>%
  ggplot(aes(x=Type,  y=value)) +
  stat_pointinterval()+
  facet_wrap(~Class,  scales='free')
```

**Conclusions:**

- in each case, the prior is substantially wider than the posterior, suggesting
  that the posterior is not biased towards the prior.
  
```{r fitNBModel2c, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
ggemmeans(peake.brm4,  ~AREA) %>% plot(add.data=TRUE)
```

### MCMC chain and posterior checks
```{r fitNBModel2d, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=4}
plot(peake.brm4, type='mcmc_trace')
plot(peake.brm4, type='acf_bar')
plot(peake.brm4, type='rhat_hist')
plot(peake.brm4, type='neff_hist')
pp_check(peake.brm4, x='AREA',type='error_scatter_avg_vs_x')
pp_check(peake.brm4, x='AREA', type='intervals')
```

### DHARMa residuals

```{r fitNBModel2e, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
preds <- posterior_predict(peake.brm4,  nsamples=250,  summary=FALSE)
peake.resids <- createDHARMa(simulatedResponse = t(preds),
                            observedResponse = peake$INDIV,
                            fittedPredictedResponse = apply(preds, 2, median),
                            integerResponse = TRUE)
plot(peake.resids)
```

### LOOIC

Different models can be compared via information criterion.
LOO (Leave One Out) IC is similar to AIC except that AIC does not consider priors
and assumes that the posterior likelihood is multivariate normal. LOO AIC does
not and integrates over all uncertainty.

- ELPD: is the theoretical expected log pointwise predictive density for a
  new dataset and can be estimated via leave-one-out cross-validation.
- `elpd_loo`: is the Bayesian LOO estimate of ELPD
- `SE` of `elpd_loo`: is the Monte Carlo standard error is the estimate for the
  computational accuracy of MCMC and importance sampling used to compute
  `elpd_loo`
- `p_loo`: is the difference between `elpd_loo` and the estimated from
  non-cross validated ELPD and therefore is a measure of the relative extra
  difficulty of predicting new data compared to the observed data.  It can also
  be a meaure of the effective number of parameters:
  - in a well behaved model, `p_loo` will be less than the number of
    estimated parameters and the total sample size.
  - in a misspecified model, `p_loo` will be greater than the number of
    estimated parameters and the total sample size.
- `Pareto k`: is a relative measure of the influence of each observation as well
  as the accuracy with which the associated leave one out calculations can be
  estimated.
  - when `k < 0.5`: the corresponding components can be accurately estimated
  - when `0.5 < k < 0.7`: the accuracy is lower but still acceptable
  - when `k>0.7`: the accuracy is too low and `elpd_loo` is unreliable.  This
    can also suggest a misspecified model.

More information about the above can be gleaned from [https://mc-stan.org/loo/reference/loo-glossary.html].


Start with the Poisson model
```{r fitNBModel2f, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
(peake.brm3.loo =loo(peake.brm3))
```

**Conclusions:**

- `p_loo` is substantially higher than the number of estimated parameters
- there are some `Pareto k` values identified as bad or even very bad

Now for the Negative Binomial model
```{r fitNBModel2g, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
(peake.brm4.loo =loo(peake.brm4))
```

**Conclusions:**

- `p_loo` is lower than the number of estimated parameters
- there are no bad `Pareto k` values

We can also compare the models.

The difference in Expected Log Pointwise Probability Density (`elpd_dff`)
computes the difference in `elpd_loo` of two models (expressed relative to the
higher model).
The difference in `elpd_diff` will be negative if the
expected out-of-sample predictive accuracy of the first model is
higher. If the difference is be positive then the second model is
preferred.

Note, the above assumes that both models are valid.  In the current case, we
know that the Poisson model was overdispersed and thus it should not be a
genuine candidate.  Nevertheless, we will compare the Poisson to the Negative
Binomial for illustrative purposes.

```{r fitNBModel2h, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
loo_compare(peake.brm3.loo, peake.brm4.loo)
```

**Conclusions:**

- the `elpd_diff` is negative, indicating that the first model (Negative
  Binomial) is better.

</div>

# Partial effects plots {.tabset .tabset-faded}

<div class='HIDDEN'>

## rstanarm {.tabset .tabset-pills}

### ggpredict

```{r partialPlot1a, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
peake.rstanarm4 %>% ggpredict() %>% plot(add.data=TRUE)
```


### ggemmeans

```{r partialPlot1b, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
peake.rstanarm4 %>% ggemmeans(~AREA,  type='fixed', transform='response') %>% plot(add.data=TRUE)
```

### fitted_draws

```{r partialPlot1c, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
peake.rstanarm4 %>% fitted_draws(newdata=peake) %>%
  median_hdci() %>%
  ggplot(aes(x=AREA, y=.value)) +
  geom_ribbon(aes(ymin=.lower, ymax=.upper), fill='blue', alpha=0.3) + 
  geom_line() +
  geom_point(data=peake,  aes(y=INDIV,  x=AREA))
```

## brms {.tabset .tabset-pills}

### conditional_effects

```{r partialPlot2d, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
peake.brm4 %>% conditional_effects() 
peake.brm4 %>% conditional_effects(spaghetti=TRUE,nsamples=200) 
```

### ggpredict

```{r partialPlot2a, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
peake.brm4 %>% ggpredict() %>% plot(add.data=TRUE)
```


### ggemmeans

```{r partialPlot2b, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
peake.brm4 %>% ggemmeans(~AREA) %>% plot(add.data=TRUE)
```

### fitted_draws

```{r partialPlot2c, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
peake.brm4 %>% fitted_draws(newdata=peake) %>%
  median_hdci() %>%
  ggplot(aes(x=AREA, y=.value)) +
  geom_ribbon(aes(ymin=.lower, ymax=.upper), fill='blue', alpha=0.3) + 
  geom_line() +
  geom_point(data=peake,  aes(y=INDIV,  x=AREA))
```

</div>

# Model investigation {.tabset .tabset-faded}

<div class='HIDDEN'>


## rstanarm {.tabset .tabset-pills}

`rstanarm` captures the MCMC samples from `stan` within the returned list.
There are numerous ways to retrieve and summarise these samples.  The first
three provide convenient numeric summaries from which you can draw conclusions,
the last four provide ways of obtaining the full posteriors. 

### summary

The `summary()` method generates simple summaries (mean, standard deviation as
well as 10, 50 and 90 percentiles).

```{r summariseModel1a, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
summary(peake.rstanarm4)
```

```{r summariseModel1a1, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5, echo=FALSE}
peake.sum <- summary(peake.rstanarm4)
```

**Conclusions:**

- in the Model Info, we are informed that the total MCMC posterior sample size
  is `r nrow(as.matrix(peake.rstanarm4))` and that there were 19 raw observations.
- the estimated intercept (expected number of individuals for a mussel clump of
  log-transformed area = 0) is
  `r round(peake.sum[1,1],2)`.  This is the mean of the posterior distribution
  for this parameter.  If we back-transform this to the response scale, this
  becomes `r round(exp(peake.sum[1, 1]),2)`.  When the mussel clump is 0, we
  expect to find `r round(exp(peake.sum[1, 1]),2)` individuals in the mussel
  clump (which obviously does not exist if it has a log-transformed area of 0!).  
- the estimated slope (rate at which the number of individuals changes per 1 unit change in
  log-transformed area), is `r round(peake.sum[2,1],2)` (mean) or 
  `r round(peake.sum[2,4],2)` (median) with a standard deviation of `r round(peake.sum[2,2],2)`.
  The 90% credibility intervals indicate that we are 90% confident that the slope is between 
  `r round(peake.sum[2,1],2)` and `r round(peake.sum[2,5],2)` - e.g. there is a
  significant positive trend.  When back-transformed onto the reponse scale, we see
  that for a one unit increase in log-transformed mussel clump area, the number
  of individuals increases by a factor of `r round(exp(peake.sum[2,1]),2)`.
  This represents a `r round(100*(exp(peake.sum[2, 1])-1), 0)`% increase per 1
  unit change in log-transformed mussel clump area.
- Rhat and number of effective samples for each parameter are also provided as
  MCMC diagnostics and all look good.


### tidyMCMC

```{r summariseModel1b, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
tidyMCMC(peake.rstanarm4$stanfit, estimate.method='median',  conf.int=TRUE,  conf.method='HPDinterval',  rhat=TRUE, ess=TRUE)
```
```{r summariseModel1b1, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5,echo=FALSE}
peake.tidy <- tidyMCMC(peake.rstanarm4$stanfit, estimate.method='median',  conf.int=TRUE,  conf.method='HPDinterval',  rhat=TRUE, ess=TRUE)
```

**Conclusions:**

- the estimated intercept (expected number of individuals for a mussel clump of
  log-transformed area = 0) is
  `r round(as.numeric(peake.tidy[1,2]),2)`.  This is the median of the posterior distribution
  for this parameter.  If we back-transform this to the response scale, this
  becomes `r round(exp(as.numeric(peake.tidy[1, 2])),2)`.  When the mussel clump is 0, we
  expect to find `r round(exp(as.numeric(peake.tidy[1, 2])),2)` individuals in the mussel
  clump (which obviously does not exist if it has a log-transformed area of 0!).  
- the estimated slope (rate at which the number of individuals changes per 1 unit change in
  log-transformed area), is `r round(as.numeric(peake.tidy[2,2]),2)` (mean) with
  a standard error of `r round(as.numeric(peake.tidy[2,3]),2)`.
  The 95% credibility intervals indicate that we are 90% confident that the slope is between 
  `r round(as.numeric(peake.tidy[2,4]),2)` and `r round(as.numeric(peake.tidy[2,5]),2)` - e.g. there is a
  significant positive trend.  When back-transformed onto the reponse scale, we see
  that for a one unit increase in log-transformed mussel clump area, the number
  of individuals increases by a factor of `r round(exp(as.numeric(peake.tidy[2,2])),2)`.
  This represents a `r round(100*(exp(as.numeric(peake.tidy[2, 2]))-1), 0)`% increase per 1
  unit change in log-transformed mussel clump area.
- Rhat and number of effective samples for each parameter are also provided as
  MCMC diagnostics and all look good.

### gather_draws

Due to the presence of a log transform in the predictor, it is better to use the
regex version.
```{r summariseModel1c, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
peake.rstanarm4 %>% get_variables()
peake.draw <- peake.rstanarm4 %>% gather_draws(`.Intercept.*|.*AREA.*`,  regex=TRUE)
peake.draw
```

We can then summarise this

```{r summariseModel1c1, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
peake.draw %>% median_hdci
```

```{r summariseModel1c3, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5,echo=FALSE}
peake.gather <- peake.rstanarm4 %>% gather_draws(`.Intercept.*|.*AREA.*`,  regex=TRUE) %>%
  median_hdci
```

```{r summariseModel1c4, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5,echo=TRUE}
peake.rstanarm4 %>% 
  gather_draws(`.Intercept.*|.*AREA.*`, regex=TRUE) %>% 
  ggplot() + 
  stat_halfeye(aes(x=.value,  y=.variable)) +
  facet_wrap(~.variable, scales='free')
```
 
We could alternatively express the parameters on the response scale.
```{r summariseModel1c5, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5,echo=TRUE}
peake.rstanarm4 %>% 
  gather_draws(`.Intercept.*|.*AREA.*`, regex=TRUE) %>%
  group_by(.variable) %>%
  mutate(.value=exp(.value)) %>%
  median_hdci
```

**Conclusions:**

- the estimated intercept (expected number of individuals for a mussel clump of
  log-transformed area = 0) is
  `r round(as.numeric(peake.gather[1,2]),2)`.  This is the median of the posterior distribution
  for this parameter.  If we back-transform this to the response scale, this
  becomes `r round(exp(as.numeric(peake.gather[1, 2])),2)`.  When the mussel clump is 0, we
  expect to find `r round(exp(as.numeric(peake.gather[1, 2])),2)` individuals in the mussel
  clump (which obviously does not exist if it has a log-transformed area of 0!).  
- the estimated slope (rate at which the number of individuals changes per 1 unit change in
  log-transformed area), is `r round(as.numeric(peake.gather[2,2]),2)` (mean) with
  a standard error of `r round(as.numeric(peake.gather[2,3]),2)`.
  The 95% credibility intervals indicate that we are 90% confident that the slope is between 
  `r round(as.numeric(peake.gather[2,4]),2)` and `r round(as.numeric(peake.gather[2,5]),2)` - e.g. there is a
  significant positive trend.  When back-transformed onto the reponse scale, we see
  that for a one unit increase in log-transformed mussel clump area, the number
  of individuals increases by a factor of `r round(exp(as.numeric(peake.gather[2,2])),2)`.
  This represents a `r round(100*(exp(as.numeric(peake.gather[2, 2]))-1), 0)`% increase per 1
  unit change in log-transformed mussel clump area.
- Rhat and number of effective samples for each parameter are also provided as
  MCMC diagnostics and all look good.

### bayesplot

```{r summariseModel1j, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
peake.rstanarm4 %>% plot(plotfun='mcmc_intervals') 
```

### tidy_draws

This is purely a graphical depiction on the posteriors.

```{r summariseModel1d, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
peake.rstanarm4 %>% tidy_draws()
```

### spread_draws

```{r summariseModel1e, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
peake.rstanarm4 %>% spread_draws(`.Intercept.*|.*AREA.*`,  regex=TRUE)
```

### posterior_samples
```{r summariseModel1f, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
peake.rstanarm4 %>% posterior_samples() %>% as_tibble()
```

### $R^2$

Unfortunately, $R^2$ calculations for models other than Gaussian and Binomial
have not yet been implemented for `rstanarm` models yet.

```{r summariseModel1g, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
#peake.rstanarm4 %>% bayes_R2() %>% median_hdci
```

## brms {.tabset .tabset-pills}

`brms` captures the MCMC samples from `stan` within the returned list.
There are numerous ways to retrieve and summarise these samples.  The first
three provide convenient numeric summaries from which you can draw conclusions,
the last four provide ways of obtaining the full posteriors. 

### summary

The `summary()` method generates simple summaries (mean, standard deviation as
well as 10, 50 and 90 percentiles).

```{r summariseModel2a, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
summary(peake.brm4)
```

```{r summariseModel2a1, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5, echo=FALSE}
peake.sum <- summary(peake.brm4)
```

**Conclusions:**

- in the Model Info, we are informed that the total MCMC posterior sample size
  is `r nrow(as.matrix(peake.brm4))` and that there were 19 raw observations.
- the estimated intercept (expected number of individuals for a mussel clump of
  log-transformed area = 0) is
  `r round(peake.sum$fixed[1,1],2)`.  This is the mean of the posterior distribution
  for this parameter.  If we back-transform this to the response scale, this
  becomes `r round(exp(peake.sum$fixed[1, 1]),2)`.  When the mussel clump is 0, we
  expect to find `r round(exp(peake.sum$fixed[1, 1]),2)` individuals in the mussel
  clump (which obviously does not exist if it has a log-transformed area of 0!).  
- the estimated slope (rate at which the number of individuals changes per 1 unit change in
  log-transformed area), is `r round(peake.sum$fixed[2,1],2)` (mean) or 
  `r round(peake.sum$fixed[2,4],2)` (median) with a standard deviation of `r round(peake.sum$fixed[2,2],2)`.
  The 90% credibility intervals indicate that we are 90% confident that the slope is between 
  `r round(peake.sum$fixed[2,1],2)` and `r round(peake.sum$fixed[2,5],2)` - e.g. there is a
  significant positive trend.  When back-transformed onto the reponse scale, we see
  that for a one unit increase in log-transformed mussel clump area, the number
  of individuals increases by a factor of `r round(exp(peake.sum$fixed[2,1]),2)`.
  This represents a `r round(100*(exp(peake.sum$fixed[2, 1])-1), 0)`% increase per 1
  unit change in log-transformed mussel clump area.
- Rhat and number of effective samples for each parameter are also provided as
  MCMC diagnostics and all look good.


### tidyMCMC

```{r summariseModel2b, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
tidyMCMC(peake.brm4$fit, estimate.method='median',  conf.int=TRUE,  conf.method='HPDinterval',  rhat=TRUE, ess=TRUE)
```
```{r summariseModel2b1, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5,echo=FALSE}
peake.tidy <- tidyMCMC(peake.brm4$fit, estimate.method='median',  conf.int=TRUE,  conf.method='HPDinterval',  rhat=TRUE, ess=TRUE)
```

**Conclusions:**

- the estimated intercept (expected number of individuals for a mussel clump of
  log-transformed area = 0) is
  `r round(as.numeric(peake.tidy[1,2]),2)`.  This is the median of the posterior distribution
  for this parameter.  If we back-transform this to the response scale, this
  becomes `r round(exp(as.numeric(peake.tidy[1, 2])),2)`.  When the mussel clump is 0, we
  expect to find `r round(exp(as.numeric(peake.tidy[1, 2])),2)` individuals in the mussel
  clump (which obviously does not exist if it has a log-transformed area of 0!).  
- the estimated slope (rate at which the number of individuals changes per 1 unit change in
  log-transformed area), is `r round(as.numeric(peake.tidy[2,2]),2)` (mean) with
  a standard error of `r round(as.numeric(peake.tidy[2,3]),2)`.
  The 95% credibility intervals indicate that we are 90% confident that the slope is between 
  `r round(as.numeric(peake.tidy[2,4]),2)` and `r round(as.numeric(peake.tidy[2,5]),2)` - e.g. there is a
  significant positive trend.  When back-transformed onto the reponse scale, we see
  that for a one unit increase in log-transformed mussel clump area, the number
  of individuals increases by a factor of `r round(exp(as.numeric(peake.tidy[2,2])),2)`.
  This represents a `r round(100*(exp(as.numeric(peake.tidy[2, 2]))-1), 0)`% increase per 1
  unit change in log-transformed mussel clump area.
- Rhat and number of effective samples for each parameter are also provided as
  MCMC diagnostics and all look good.

### gather_draws

Due to the presence of a log transform in the predictor, it is better to use the
regex version.
```{r summariseModel2c, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
peake.brm4 %>% get_variables()
peake.draw <- peake.brm4 %>% gather_draws(`b_.*`,  regex=TRUE)
peake.draw
```

We can then summarise this

```{r summariseModel2c1, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
peake.draw %>% median_hdci
```

```{r summariseModel2c3, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5,echo=FALSE}
peake.gather <- peake.brm4 %>% gather_draws(`b_.*`,  regex=TRUE) %>%
  median_hdci
```

```{r summariseModel2c4, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5,echo=TRUE}
peake.brm4 %>% 
  gather_draws(`b_.*`, regex=TRUE) %>% 
  ggplot() + 
  stat_halfeye(aes(x=.value,  y=.variable)) +
  facet_wrap(~.variable, scales='free')
```
 
We could alternatively express the parameters on the response scale.
```{r summariseModel2c5, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5,echo=TRUE}
peake.brm4 %>% 
  gather_draws(`.Intercept.*|.*AREA.*`, regex=TRUE) %>%
  group_by(.variable) %>%
  mutate(.value=exp(.value)) %>%
  median_hdci
```

**Conclusions:**

- the estimated intercept (expected number of individuals for a mussel clump of
  log-transformed area = 0) is
  `r round(as.numeric(peake.gather[1,2]),2)`.  This is the median of the posterior distribution
  for this parameter.  If we back-transform this to the response scale, this
  becomes `r round(exp(as.numeric(peake.gather[1, 2])),2)`.  When the mussel clump is 0, we
  expect to find `r round(exp(as.numeric(peake.gather[1, 2])),2)` individuals in the mussel
  clump (which obviously does not exist if it has a log-transformed area of 0!).  
- the estimated slope (rate at which the number of individuals changes per 1 unit change in
  log-transformed area), is `r round(as.numeric(peake.gather[2,2]),2)` (mean) with
  a standard error of `r round(as.numeric(peake.gather[2,3]),2)`.
  The 95% credibility intervals indicate that we are 90% confident that the slope is between 
  `r round(as.numeric(peake.gather[2,4]),2)` and `r round(as.numeric(peake.gather[2,5]),2)` - e.g. there is a
  significant positive trend.  When back-transformed onto the reponse scale, we see
  that for a one unit increase in log-transformed mussel clump area, the number
  of individuals increases by a factor of `r round(exp(as.numeric(peake.gather[2,2])),2)`.
  This represents a `r round(100*(exp(as.numeric(peake.gather[2, 2]))-1), 0)`% increase per 1
  unit change in log-transformed mussel clump area.
- Rhat and number of effective samples for each parameter are also provided as
  MCMC diagnostics and all look good.

### bayesplot

```{r summariseModel2j, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
peake.brm4 %>% plot(plotfun='mcmc_intervals') 
```

### tidy_draws

This is purely a graphical depiction on the posteriors.

```{r summariseModel2d, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
peake.brm4 %>% tidy_draws()
```

### spread_draws

```{r summariseModel2e, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
peake.brm4 %>% spread_draws(`b_.*`,  regex=TRUE)
```

### posterior_samples
```{r summariseModel2f, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
peake.brm4 %>% posterior_samples() %>% as_tibble()
```

### $R^2$


```{r summariseModel2g, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
peake.brm4 %>% bayes_R2(summary=FALSE) %>% median_hdci
```
</div>


# Hypothesis testing {.tabset .tabset-faded}

<div class='HIDDEN'>
Since we have the entire posterior, we are able to make probablity statements.
We simply count up the number of MCMC sample draws that satisfy a condition (e.g
represent a slope greater than 0) and then divide by the total number of MCMC
samples.

For this exersize, we will explore the following:

- what this the probability that the number of individuals increases with
  increasing mussel clump area - this is just asking what proportion of the
  drawn slopes are greater than 0.
- how much of a change in individuals do we expect if the mussel clump area is
  increased from 5,000 to 10,000 units and what is the probability that the change is
  more than 50%
  
## rstanarm {.tabset .tabset-pills}

### Prob. of effect

Unfortunately, the inline log-transformation of the predictor interfers with
`hypothesis()`'s ability to find the slope.  We can get around this by renaming
before calling `hypothesis()`.

```{r Probability1a, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
peake.rstanarm4 %>% as.data.frame() %>% rename(lAREA=`log(AREA)`) %>% hypothesis('lAREA>0')
```

**Conclusions:**

- the probability of a positive effect of mussel clump area on number of individuals is 1.  That
  is, there is, we are 100% confident that there is an effect.
- the evidence ratio is normally the ratio of the number of cases that satisty
  the hypothesis to the number of cases that do not.  Since the number of cases
  that do not satisfy the hypothesis is 0, the evidence ratio is Inf (since
  division by 0)

Alternatively...

```{r Probability1b, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5,paged.print=FALSE}
peake.rstanarm4 %>% tidy_draws() %>% summarise(P=sum(`log(AREA)`>0)/n())
```

**Conclusions:**

- the probability of a positive effect of mussel clump area on number of individuals is 1.  That
  is, there is, we are 100% confident that there is an effect.


### Change 5000 to 10000

```{r Probability1c, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5, echo=1:2}
newdata = list(AREA=c(5000, 10000)) 
peake.rstanarm4 %>% emmeans(~AREA,  at=newdata) %>% pairs()
peake.mcmc <- peake.rstanarm4 %>% emmeans(~AREA,  at=newdata) %>% pairs() %>% as.data.frame()
```

**Conclusions:**

- the change in expected number of individuals associated with an increase in
  mussel clump area from 5,000 to 10,000 is `r peake.mcmc[1, 2] %>% round(2)`.

If we want to derive other properties, such as the percentage change, then we
use `tidy_draws()` and then simple `tidyverse` spreadsheet operation.

```{r Probability1d, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
peake.mcmc <- peake.rstanarm4 %>% emmeans(~AREA,  at=newdata) %>% 
  tidy_draws() %>%
  rename_with(~str_replace(., 'AREA ', 'p')) %>%
  mutate(Eff=p10000 - p5000,
         PEff=100*Eff/p5000)
peake.mcmc %>% head
```

Now we can calculate the medians and HPD intervals of each column (and ignore
the `.chain`, `.iteration` and `.draw`).

```{r Probability1e, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
peake.mcmc %>% tidyMCMC(estimate.method='median',
                       conf.int=TRUE, conf.method='HPDinterval')
```

Alternatively, we could use `median_hdci`

```{r Probability1f, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
peake.mcmc %>% median_hdci(PEff)
```

**Conclusions:**

- the percentage change in expected number of individuals associated with an
  increase in mussel clump area from 5,000 to 10,000 is `r peake.mcmc %>% median_hdci(PEff) %>% '['(1, 1) %>% as.numeric %>% round(2)`% 
  with a 95% credibility interval of 
  `r peake.mcmc %>% median_hdci(PEff) %>% '['(1, 2) %>% as.numeric %>% round(2)`
  `r peake.mcmc %>% median_hdci(PEff) %>% '['(1, 3) %>% as.numeric %>% round(2)`

To get the probability that the effect is greater than a 50% increase.
```{r Probability1g, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5, paged.print=FALSE}
peake.mcmc %>% summarise(P=sum(PEff>50)/n())
```

**Conclusions:**

- the probability that the number of individuals will increase by more than 50% following an increase
  in mussel clump area from 5,000 to 10,000 is
  `r peake.mcmc %>% summarise(P=sum(PEff>50)/n()) %>% as.numeric() %>% round(2)`.


Finally, we could alternatively use `hypothesis()`. Note that when we do so, the
estimate is the difference between the effect and the hypothesised effect (50%).

```{r Probability1h, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
peake.mcmc %>% hypothesis('PEff>50')
```

## brm {.tabset .tabset-pills}

### Prob. of effect

Unfortunately, the inline log-transformation of the predictor interfers with
`hypothesis()`'s ability to find the slope.

```{r Probability2a, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
peake.brm4 %>% hypothesis('logAREA>0')
```

**Conclusions:**

- the probability of a positive effect of mussel clump area on number of individuals is 1.  That
  is, there is, we are 100% confident that there is an effect.
- the evidence ratio is normally the ratio of the number of cases that satisty
  the hypothesis to the number of cases that do not.  Since the number of cases
  that do not satisfy the hypothesis is 0, the evidence ratio is Inf (since
  division by 0)

Alternatively...

```{r Probability2b, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5,paged.print=FALSE}
peake.brm4 %>% tidy_draws() %>% summarise(P=sum(b_logAREA>0)/n())
```

**Conclusions:**

- the probability of a positive effect of mussel clump area on number of individuals is 1.  That
  is, there is, we are 100% confident that there is an effect.


### Change 5000 to 10000

```{r Probability2c, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5, echo=1:2}
newdata = list(AREA=c(5000, 10000)) 
peake.brm4 %>% emmeans(~AREA,  at=newdata) %>% pairs()
peake.mcmc <- peake.brm4 %>% emmeans(~AREA,  at=newdata) %>% pairs() %>% as.data.frame()
```

**Conclusions:**

- the change in expected number of individuals associated with an increase in
  mussel clump area from 5,000 to 10,000 is `r peake.mcmc[1, 2] %>% round(2)`.

If we want to derive other properties, such as the percentage change, then we
use `tidy_draws()` and then simple `tidyverse` spreadsheet operation.

```{r Probability2d, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
peake.mcmc <- peake.brm4 %>% emmeans(~AREA,  at=newdata) %>% 
  tidy_draws() %>%
  rename_with(~str_replace(., 'AREA ', 'p')) %>%
  mutate(Eff=p10000 - p5000,
         PEff=100*Eff/p5000)
peake.mcmc %>% head
```

Now we can calculate the medians and HPD intervals of each column (and ignore
the `.chain`, `.iteration` and `.draw`).

```{r Probability2e, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
peake.mcmc %>% tidyMCMC(estimate.method='median',
                       conf.int=TRUE, conf.method='HPDinterval')
```

Alternatively, we could use `median_hdci`

```{r Probability2f, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
peake.mcmc %>% median_hdci(PEff)
```

**Conclusions:**

- the percentage change in expected number of individuals associated with an
  increase in mussel clump area from 5,000 to 10,000 is `r peake.mcmc %>% median_hdci(PEff) %>% '['(1, 1) %>% as.numeric %>% round(2)`% 
  with a 95% credibility interval of 
  `r peake.mcmc %>% median_hdci(PEff) %>% '['(1, 2) %>% as.numeric %>% round(2)`
  `r peake.mcmc %>% median_hdci(PEff) %>% '['(1, 3) %>% as.numeric %>% round(2)`

To get the probability that the effect is greater than a 50% increase.
```{r Probability2g, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5, paged.print=FALSE}
peake.mcmc %>% summarise(P=sum(PEff>50)/n())
```

**Conclusions:**

- the probability that the number of individuals will increase by more than 50% following an increase
  in mussel clump area from 5,000 to 10,000 is
  `r peake.mcmc %>% summarise(P=sum(PEff>50)/n()) %>% as.numeric() %>% round(2)`.


Finally, we could alternatively use `hypothesis()`. Note that when we do so, the
estimate is the difference between the effect and the hypothesised effect (50%).

```{r Probability2h, results='markdown', eval=TRUE, hidden=TRUE, fig.width=8, fig.height=5}
peake.mcmc %>% hypothesis('PEff>50')
```
</div>

# Summary figure {.tabset .tabset-faded}

<div class='HIDDEN'>

## rstanarm {.tabset .tabset-pills}

```{r figureModel1a, results='markdown', eval=TRUE, hidden=TRUE}
## Using emmeans
peake.grid = with(peake, list(AREA = seq(min(AREA), max(AREA), len=100)))

newdata = emmeans(peake.rstanarm4, ~AREA, at=peake.grid, type='response') %>% as.data.frame
head(newdata)

ggplot(newdata, aes(y=prob, x=AREA)) + 
geom_point(data=peake, aes(y=INDIV)) +
geom_line() + 
geom_ribbon(aes(ymin=lower.HPD, ymax=upper.HPD), fill='blue', alpha=0.3) +
scale_y_continuous('Individuals') +
scale_x_continuous('Mussel clump area') +
    theme_classic()
```

## brms {.tabset .tabset-pills}

```{r figureModel2a, results='markdown', eval=TRUE, hidden=TRUE}
## Using emmeans
peake.grid = with(peake, list(AREA = seq(min(AREA), max(AREA), len=100)))

newdata = emmeans(peake.brm4, ~AREA, at=peake.grid, type='response') %>% as.data.frame
head(newdata)

ggplot(newdata, aes(y=prob, x=AREA)) + 
geom_point(data=peake, aes(y=INDIV)) +
geom_line() + 
geom_ribbon(aes(ymin=lower.HPD, ymax=upper.HPD), fill='blue', alpha=0.3) +
scale_y_continuous('Individuals') +
scale_x_continuous('Mussel clump area') +
    theme_classic()
```

</div>

# References
