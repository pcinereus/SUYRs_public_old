---
title: "Regression Trees Part1"
author: "Murray Logan"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: show
    collapse: no
    df_print: paged
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    highlight: textmate
    theme: spacelab
    toc: yes
    toc_float: yes
    css: ../public/resources/ws_style.css
  pdf_document:
    df_print: default
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    highlight: tango
    latex_engine: xelatex
    number_sections: yes
    toc_depth: 2
  word_document:
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    highlight: tango
    toc: yes
    toc_depth: 2
output_dir: "docs"
documentclass: article
fontsize: 12pt
mainfont: Arial
mathfont: LiberationMono
monofont: DejaVu Sans Mono
classoption: a4paper
bibliography: ../public/resources/references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

# Preparations

https://besjournals.onlinelibrary.wiley.com/doi/10.1111/j.1365-2656.2008.01390.x

Load the necessary libraries

```{r libraries, results='markdown', eval=TRUE}
library(gbm)         #for gradient boosted models
library(car)
library(dismo)
library(pdp)
library(ggfortify)
library(randomForest)
library(tidyverse)
library(gridExtra)
library(patchwork)
```

# Scenario

Abalone are an important aquaculture shell fish that are farmed
for both their meat and their shells.  Abalone can live up to 50 
years, although their longevity is known to be influenced by a 
range of environmental factors.  Traditionally, abalone are aged
by counting thier growth rings, however, this method is very
laborious and expensive.  Hence a study was conducted in which abalone
growth ring counts were matched up with a range of other more easily
measured physical characteristics (such as shell dimesions and weights)
in order to see if any of these other parameters could be used as
proxies for the number of growth rings (or age).

![abalone](../public/resources/abalone.jpg){width="251" height="290"}

Format of abalone.csv data file


# Read in the data

```{r readData, results='markdown', eval=TRUE}
abalone = read_csv('../public/data/abalone.csv', trim_ws=TRUE)
glimpse(abalone)
```

```{r preparation, results='markdown', eval=TRUE, hidden=TRUE}
abalone = abalone %>% mutate(SEX=factor(SEX))
```


# Exploratory data analysis

<div class='HIDDEN'>

```{r EDA, results='markdown', eval=TRUE, hidden=TRUE, fig.width=15, fig.height=15}
ggplot(abalone) +
    geom_point(aes(y=AGE, x=RINGS))
## Very tight relationship

scatterplotMatrix(~RINGS + SEX +LENGTH+DIAMETER+HEIGHT+WHOLE_WEIGHT+MEAT_WEIGHT+GUT_WEIGHT+
                  SHELL_WEIGHT, data=abalone)

```

</div>

# Fit the model

<div class='HIDDEN'>

We will start by partitioning the data into training and test sets.  The purpose
of partitioning is to provide one large set of data that can be used to train the
regression trees and another smaller one that can be used to evaluate its accuracy.

```{r fitModel1, results='markdown', eval=TRUE, hidden=TRUE}
set.seed(123)
nrow(abalone)
i = sample(1:nrow(abalone), 100,replace=FALSE)
abalone.train = abalone[-i,]
abalone.test = abalone[i,]
```

Now we will specify the gradient boosted model with:

- Poisson distribution.  This is not the same as the usual 'family'.  It does
  not directly nominate a modelling family.  Instead it determines the **loss**
  function that is used to evaluate the tree splits.
- **var.monotone**: indicates which of the predictors should be constrained to
  monotonic trends (0: not monotonic, 1: positive monotonic, -1: negative
  monotonic)
- **n.trees**: the total number of trees to sequentially fit
- **interaction.depth**: the number of splits within each tree.  This roughly
  equates to the order of interactions.  The higher the number, the more complex
  the interactions can be, however, the faster the potential rate of learning
- **bag.fraction**: random fraction of data used to fit a single tree
- **shrinkage**: rate of learning.  The smaller the value, the slower the
  learning rate.  Typically, values should be between 0.001 and 0.01.
- **train.fraction**: the proportion of the data used to train the tree. This is
  for the purpose of determining the balance between over and underfitting and
  evaluate the accuracy of the fit, however for the former purpose, this is a
  cruder approach than either out-of-bag or cross validation. Therefore, it is
  arguably better to set this fraction to 1 (use 100 percent of the data for
  training) and use either out-of-bag or cross validation and (as we have done
  here), withhold a fraction of the full data and perform our own accuracy
  checking.
- **cv.folds**: the number of cross validation folds.

The values of `n.trees`, `interaction.depth` and `shrinkage` used below are
purely based on what are typically good starting points.  Nevertheless, they
will be data set dependent.  We will start off with those values and then evaluate
whether they are appropriate.

```{r fitModel2, results='markdown', eval=TRUE, hidden=TRUE, cache=TRUE}
abalone.gbm = gbm(RINGS ~ SEX + LENGTH + DIAMETER + HEIGHT +
                      WHOLE_WEIGHT + MEAT_WEIGHT + GUT_WEIGHT +
                      SHELL_WEIGHT,
                  data=abalone,
                  distribution='poisson',
                  var.monotone=c(0,1,1,1,1,1,1,1),
                  n.trees=10000,
                  interaction.depth=5,
                  bag.fraction=0.5,
                  shrinkage=0.01,
                  train.fraction=1,
                  cv.folds=3)
```

We will now determine the optimum number of trees estimated to be required in
order to achieve a balance between bias (biased towards the exact observations)
and precision (variability in estimates). Ideally, the optimum number of trees
should be close to 1000. If it is much less (as in this case), it could imply
that the tree learned too quickly. On the other hand, if the optimum number of
trees is very close to the total number of fitted trees, then it suggests that
the optimum may not actually have occured yet and that more trees should be used
(or a faster learning rate).

```{r fitModel3, results='markdown', eval=TRUE, hidden=TRUE}
(best.iter = gbm.perf(abalone.gbm,method='OOB'))
(best.iter = gbm.perf(abalone.gbm,method='cv'))
```

**Conclusions:** 

- both out-of-bag and cross validation suggest that the total number of trees
  greatly exceeds the number necessary to achieve the optimum balance between
  bias and precision.
- it might be worth decreasing the learning rate. 
- we can probably also reduce the total number of trees (so that it runs a
  little quicker)

```{r fitModel4, results='markdown', eval=TRUE, hidden=TRUE, cache=TRUE}
abalone.gbm = gbm(RINGS ~ SEX + LENGTH + DIAMETER + HEIGHT +
                    WHOLE_WEIGHT + MEAT_WEIGHT + GUT_WEIGHT +
                    SHELL_WEIGHT,
                  data=abalone,
                  distribution='poisson',
                  var.monotone=c(0,1,1,1,1,1,1,1),
                  n.trees=5000,
                  interaction.depth=5,
                  bag.fraction=0.5,
                  shrinkage=0.001,
                  train.fraction=1,
                  cv.folds=3)
```

```{r fitModel5, results='markdown', eval=TRUE, hidden=TRUE}
(best.iter = gbm.perf(abalone.gbm,method='OOB'))
(best.iter = gbm.perf(abalone.gbm,method='cv'))
```

**Conclusions:** 

- both out-of-bag and cross validation suggest that the total number of trees
  greatly exceeds the number necessary to achieve the optimum balance between
  bias and precision.
- it might be worth decreasing the learning rate. 

</div>

# Explore relative influence

<div class='HIDDEN'>

If a predictor is an important driver of the patterns in the response, then many
of the tree splits should feature this predictor.  It thus follows that the
number of proportion of total splits that features each predictor will be a
measure of the relative influence of each of the predictors.

```{r relativeInfluence1, results='markdown', eval=TRUE, hidden=TRUE, fig.width=7, fig.height=10}
summary(abalone.gbm, n.trees=best.iter)
```

**Conclusions:**

- shell weight is overwelmingly the most influential predictor.
- we can also ascribe a kind of signficance to the influence values by
  determining which of them are greater than $100/p$ where $p$ is the number of
  predictors.  The logic here is that if all predictors were equally
  influential, then they would all have a relative influence of $100/p$.  Hence,
  any predictors that have a relative influence greater than this number must be
  explaining more than its share (and more than pure chance) and predictors with
  relative influence lower are explaining less than chance (and therefore not
  significantly influential).

</div>

# Explore partial effects

<div class='HIDDEN'>

```{r partialEffects1, results='markdown', eval=TRUE, hidden=TRUE, fig.width=6, fig.height=6}
attr(abalone.gbm$Terms,"term.labels")
plot(abalone.gbm, 8, n.tree=best.iter)
abalone.gbm %>%
    pdp::partial(pred.var='SHELL_WEIGHT',
                 n.trees=best.iter,
                 recursive=FALSE,
                 inv.link=exp) %>%
    autoplot()
```

Recursive indicates that a weighted tree traversal method described by Friedman
2001 (which is very fast) should be used (only works for gbm).  Otherwise a
slower brute force method is used. If want to back transform - need to use brute
force.

```{r partialEffects2, results='markdown', eval=TRUE, hidden=TRUE, fig.width=10, fig.height=10}
nms <- attr(abalone.gbm$Terms,"term.labels")
p <- vector('list', length(nms))
names(p) <- nms
for (nm in nms) {
  print(nm)
  p[[nm]] <- abalone.gbm %>% pdp::partial(pred.var=nm,
                                 n.trees=best.iter,
                                 inv.link=exp,
                                 recursive=FALSE,
                                 type='regression') %>%
    autoplot() + ylim(0, 20)
}
patchwork::wrap_plots(p) 
do.call('grid.arrange', p)
```

We might also want to explore interactions...

```{r partialEffects3, results='markdown', eval=TRUE, hidden=TRUE, cache=FALSE}
abalone.gbm %>%
    pdp::partial(pred.var=c('SHELL_WEIGHT'),
                 n.trees=best.iter, recursive=FALSE, inv.link=exp) %>%
    autoplot()

abalone.gbm %>%
    pdp::partial(pred.var=c('SHELL_WEIGHT','HEIGHT'),
                 n.trees=best.iter, recursive=TRUE) %>%
    autoplot()

g1 = abalone.gbm %>% pdp::partial(pred.var='SHELL_WEIGHT', n.trees=best.iter,
                            recursive=FALSE,inv.link=exp) %>%
    autoplot
g2 = abalone.gbm %>% pdp::partial(pred.var='HEIGHT', n.trees=best.iter,
                            recursive=FALSE,inv.link=exp) %>%
    autoplot


g1 + g2
```

</div>

# Explore accuracy

<div class='HIDDEN'>

```{r Accuracy1, results='markdown', eval=TRUE, hidden=TRUE, cache=FALSE, fig.width=7, fig.height=7}
abalone.acc <- abalone.test %>%
    bind_cols(Pred = predict(abalone.gbm,
                             newdata=abalone.test,
                             n.tree=best.iter,
                             type='response'))

with(abalone.acc,  cor(RINGS, Pred))


abalone.acc %>%
  ggplot() +
  geom_point(aes(y=Pred,  x=RINGS))

```

</div>

# Explore interactions {.tabset .tabset-faded}

<div class='HIDDEN'>

Computes Friedman's H-statistic to assess the strength of variable interactions.
This measures the relative strength of interactions in models
It is on a scale of 0-1, where 1 is very strong interaction
In y=β_0+β_1x_1+β_2x_2+β_3x_3..
H=\frac{β_3}{√{β_1^2+β_2^2+β_3^2}}
If both main effects are weak, then the H- stat will be unstable.. and could indicate
a strong interaction.

## One at a time

What were the strong main effects:
 - SHELL_WEIGHT
 - HEIGHT
 - SEX


```{r Interactions1, results='markdown', eval=TRUE, hidden=TRUE, cache=FALSE, fig.width=7, fig.height=7}
attr(abalone.gbm$Terms,"term.labels")
 
interact.gbm(abalone.gbm, abalone,c(1,4), n.tree=best.iter)
interact.gbm(abalone.gbm, abalone,c(4,8), n.tree=best.iter)
interact.gbm(abalone.gbm, abalone,c(4,8), n.tree=best.iter)
interact.gbm(abalone.gbm, abalone,c(1,4,8), n.tree=best.iter)

abalone.gbm %>% pdp::partial(pred.var=c(1),  n.trees=best.iter, recursive=FALSE) %>% autoplot
abalone.gbm %>% pdp::partial(pred.var=c(1, 4),  n.trees=best.iter, recursive=FALSE) %>% autoplot
abalone.gbm %>% pdp::partial(pred.var=c(1, 8),  n.trees=best.iter, recursive=FALSE) %>% autoplot

#plot(abalone.gbm, c(1,4), n.tree=best.iter)
#plot(abalone.gbm, c(5,6), n.tree=best.iter)
```

```{r Interactions2, results='markdown', eval=TRUE, hidden=TRUE, cache=FALSE, fig.width=7, fig.height=7}

abalone.grid = plot(abalone.gbm, c(1,4), n.tree=best.iter, return.grid=TRUE)
head(abalone.grid)

ggplot(abalone.grid, aes(y=HEIGHT, x=SEX)) +
    geom_tile(aes(fill=y)) +
    geom_contour(aes(z=y)) +
    scale_fill_gradientn(colors=heat.colors(10))

```

## All two way interactions
```{r Interactions3, eval=TRUE, echo=FALSE}
terms <- attr(abalone.gbm$Terms,"term.labels")
abalone.int <- NULL
for (i in 1:(length(terms)-1)) {
    for (j in (i+1):length(terms)) {
        print(paste('i=',i, ' Name = ', terms[i]))
        print(paste('j=',j, ' Name = ', terms[j]))
        abalone.int <- rbind(abalone.int,
                             data.frame(Var1=terms[i], Var2=terms[j],
                                        "H.stat"=interact.gbm(abalone.gbm, abalone,c(i,j),
                                                              n.tree=best.iter)
                                        ))
    }
}
abalone.int %>% arrange(-H.stat)
```

```{r Interactions4, eval=TRUE, echo=FALSE}
plot(abalone.gbm, c(1,4), n.tree=best.iter)
plot(abalone.gbm, c(5,6), n.tree=best.iter)

abalone.grid = plot(abalone.gbm, c(5,6), n.tree=best.iter, return.grid=TRUE)
head(abalone.grid)

ggplot(abalone.grid, aes(y=MEAT_WEIGHT, x=WHOLE_WEIGHT)) +
    geom_tile(aes(fill=y)) +
    geom_contour(aes(z=y)) +
   scale_fill_gradientn(colors=heat.colors(10))

```

</div>

# Tuning

<div class='HIDDEN'>

The takes a long time - do over a break

```{r gbmstep1, eval=TRUE, hidden=TRUE, cache=FALSE}
abalone.gbm1 <- gbm.step(data=abalone %>% as.data.frame, gbm.x=1:8, gbm.y=9,
                        tree.complexity=5,
                        learning.rate=0.001,
                        bag.fraction=0.5,
                        n.trees=10000,
                        family='poisson')
```

```{r gbmstep2, eval=TRUE, hidden=TRUE, cache=FALSE}
summary(abalone.gbm1)
```


```{r gbmstep3, eval=TRUE, hidden=TRUE, cache=FALSE, fig.width=10,  fig.height=10}
gbm.plot(abalone.gbm1, n.plots=8, write.title = FALSE)
```

```{r gbmstep4, eval=TRUE, hidden=TRUE, cache=FALSE, fig.width=10,  fig.height=10}
gbm.plot.fits(abalone.gbm1)
```


```{r gbmstep5, eval=TRUE, hidden=TRUE, cache=FALSE, fig.width=10,  fig.height=10}
find.int <- gbm.interactions(abalone.gbm1)
summary(find.int)
find.int$rank.list
gbm.perspec(abalone.gbm1,6,5)
```
  
</div>


```{r bootstrapping, results='markdown', eval=TRUE}
nBoot <- 10
abalone.pred <- with(abalone,
                     expand.grid(SHELL_WEIGHT = modelr::seq_range(SHELL_WEIGHT, n = 100),
                                SEX = levels(SEX),
                                LENGTH = NA,
                                DIAMETER = NA,
                                HEIGHT = NA,
                                WHOLE_WEIGHT = NA,
                                MEAT_WEIGHT = NA,
                                GUT_WEIGHT = NA)
                     )
abalone.list <- vector('list', nBoot) 
abalone.list
abalone.sum <- vector('list', nBoot) 
for (i in 1:nBoot) {
    print(paste0('Boot number: ', i))
    ## Create random set
    abalone.rnd <- abalone %>%
        sample_n(size = n(), replace=TRUE)
    ## Fit the trees
    abalone.gbm = gbm(RINGS ~ SEX + LENGTH + DIAMETER + HEIGHT +
                          WHOLE_WEIGHT + MEAT_WEIGHT + GUT_WEIGHT +
                          SHELL_WEIGHT,
                      data=abalone.rnd,
                      distribution='poisson',
                      var.monotone=c(0,1,1,1,1,1,1,1),
                      n.trees=5000,
                      interaction.depth=5,
                      bag.fraction=0.5,
                      shrinkage=0.001,
                      train.fraction=1,
                      cv.folds=3)
    ## Determine the best number of trees
    (best.iter = gbm.perf(abalone.gbm,method='cv'))
    ## predict based on shell weight
    fit <- predict(abalone.gbm, newdata = abalone.pred, n.trees = best.iter) %>% exp()
    abalone.list[[i]] <- data.frame(abalone.pred, Boot = i, Fit = fit)
    ## relative influence
    abalone.sum[[i]] <- summary(abalone.gbm, n.trees = best.iter)
}
abalone.fit <- do.call('rbind', abalone.list)
abalone.fit <- abalone.fit %>%
    group_by(SHELL_WEIGHT, SEX) %>%
    ## summarise(Median = median(Fit),
    ##           Lower = quantile(Fit, p=0.025),
    ##           Upper = quantile(Fit, p=0.975))
    ggdist::median_hdci(Fit)       
g1 <- abalone.fit %>% ggplot(aes(y=Fit, x=SHELL_WEIGHT, fill=SEX, color=SEX)) +
    geom_ribbon(aes(ymin=.lower, ymax=.upper), alpha=0.3, color=NA) +
    geom_line() +
    scale_fill_viridis_d() +
    scale_colour_viridis_d() +
    theme_classic()

abalone.inf <- do.call('rbind', abalone.sum)
abalone.inf <- abalone.inf %>%
    group_by(var) %>%
    ggdist::median_hdci(rel.inf)       

g2 <- abalone.inf %>% ggplot(aes(y=var, x=rel.inf)) +
    geom_vline(xintercept=12.5, linetype='dashed') +
    geom_pointrange(aes(xmin=.lower, xmax=.upper)) +
    theme_classic()

g2 + patchwork::inset_element(g1, left=0.5, bottom=0.01, right=1, top=0.7)
g1 + patchwork::inset_element(g2, left=0.5, bottom=0.01, right=1, top=0.5)
```





# Random Forest
```{r randomForest, results='markdown', eval=TRUE, hidden=TRUE}
library(randomForest)
abalone.rf = randomForest(RINGS ~ SEX + LENGTH + DIAMETER + HEIGHT +
                      WHOLE_WEIGHT + MEAT_WEIGHT + GUT_WEIGHT + SHELL_WEIGHT,
                      data=abalone, importance=TRUE,
                      ntree=1000)
abalone.imp = randomForest::importance(abalone.rf)
## Rank by either:
## *MSE (mean decrease in accuracy)
## For each tree, calculate OOB prediction error.
## This also done after permuting predictors.
## Then average diff of prediction errors for each tree
## *NodePurity (mean decrease in node impurity)
## Measure of the total decline of impurity due to each
## predictor averaged over trees
100*abalone.imp/sum(abalone.imp)
varImpPlot(abalone.rf)
## use brute force
abalone.rf %>% pdp::partial('SHELL_WEIGHT') %>% autoplot
```
